{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95c72ce",
   "metadata": {},
   "source": [
    "# Notebook 03: Data Archiving for C. elegans Motility Analysis\n",
    "\n",
    "This notebook handles:\n",
    "- Organizing and archiving processed data\n",
    "- Creating comprehensive analysis reports\n",
    "- Archiving raw data with metadata\n",
    "- Generating summary documentation\n",
    "- Preparing data packages for publication/sharing\n",
    "- Timestamped backups of analysis outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d17ff7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d3ce5",
   "metadata": {},
   "source": [
    "## 2. Configuration and Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "BASE_DIR = Path(r'C:\\Users\\MBF\\Motility_analysis')\n",
    "RAW_DATA_DIR = BASE_DIR / 'Data' / 'Raw'\n",
    "PROCESSED_DATA_DIR = BASE_DIR / 'Data' / 'Processed'\n",
    "ARCHIVE_DIR = BASE_DIR / 'Data' / 'Archive'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "\n",
    "# Create timestamp for this archiving session\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "ARCHIVE_SESSION_DIR = ARCHIVE_DIR / f'archive_{TIMESTAMP}'\n",
    "\n",
    "# Create archive directories\n",
    "ARCHIVE_SESSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(ARCHIVE_SESSION_DIR / 'processed_data').mkdir(exist_ok=True)\n",
    "(ARCHIVE_SESSION_DIR / 'results').mkdir(exist_ok=True)\n",
    "(ARCHIVE_SESSION_DIR / 'figures').mkdir(exist_ok=True)\n",
    "(ARCHIVE_SESSION_DIR / 'metadata').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Archive session directory: {ARCHIVE_SESSION_DIR}\")\n",
    "print(f\"Timestamp: {TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750bc3f",
   "metadata": {},
   "source": [
    "## 3. Archive Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy processed data files\n",
    "processed_files = list(PROCESSED_DATA_DIR.glob('*.csv'))\n",
    "\n",
    "print(f\"Archiving {len(processed_files)} processed data file(s)...\")\n",
    "\n",
    "for file_path in processed_files:\n",
    "    dest_path = ARCHIVE_SESSION_DIR / 'processed_data' / file_path.name\n",
    "    shutil.copy2(file_path, dest_path)\n",
    "    print(f\"  ✓ Archived: {file_path.name}\")\n",
    "\n",
    "print(\"\\nProcessed data archived successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d9b24",
   "metadata": {},
   "source": [
    "## 4. Archive Results and Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result CSV files\n",
    "result_files = [f for f in RESULTS_DIR.glob('*.csv') if f.is_file()]\n",
    "json_files = [f for f in RESULTS_DIR.glob('*.json') if f.is_file()]\n",
    "\n",
    "print(f\"Archiving {len(result_files)} result file(s)...\")\n",
    "for file_path in result_files:\n",
    "    dest_path = ARCHIVE_SESSION_DIR / 'results' / file_path.name\n",
    "    shutil.copy2(file_path, dest_path)\n",
    "    print(f\"  ✓ Archived: {file_path.name}\")\n",
    "\n",
    "print(f\"\\nArchiving {len(json_files)} JSON file(s)...\")\n",
    "for file_path in json_files:\n",
    "    dest_path = ARCHIVE_SESSION_DIR / 'results' / file_path.name\n",
    "    shutil.copy2(file_path, dest_path)\n",
    "    print(f\"  ✓ Archived: {file_path.name}\")\n",
    "\n",
    "# Copy figures\n",
    "if FIGURES_DIR.exists():\n",
    "    figure_files = list(FIGURES_DIR.glob('*'))\n",
    "    print(f\"\\nArchiving {len(figure_files)} figure(s)...\")\n",
    "    for file_path in figure_files:\n",
    "        if file_path.is_file():\n",
    "            dest_path = ARCHIVE_SESSION_DIR / 'figures' / file_path.name\n",
    "            shutil.copy2(file_path, dest_path)\n",
    "            print(f\"  ✓ Archived: {file_path.name}\")\n",
    "\n",
    "print(\"\\nResults and figures archived successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172b5a4",
   "metadata": {},
   "source": [
    "## 5. Collect and Archive Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all metadata YAML files from raw data\n",
    "metadata_files = list(RAW_DATA_DIR.rglob('metadata_*.yaml'))\n",
    "\n",
    "print(f\"Found {len(metadata_files)} metadata file(s)\")\n",
    "\n",
    "# Copy metadata files to archive with organized structure\n",
    "for metadata_path in metadata_files:\n",
    "    # Create a readable name based on path\n",
    "    relative_path = metadata_path.relative_to(RAW_DATA_DIR)\n",
    "    # Replace path separators with underscores for flat structure\n",
    "    archive_name = str(relative_path).replace('\\\\', '_').replace('/', '_')\n",
    "    \n",
    "    dest_path = ARCHIVE_SESSION_DIR / 'metadata' / archive_name\n",
    "    shutil.copy2(metadata_path, dest_path)\n",
    "    print(f\"  ✓ Archived: {archive_name}\")\n",
    "\n",
    "print(\"\\nMetadata archived successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06daf281",
   "metadata": {},
   "source": [
    "## 6. Generate Analysis Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c11736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data to generate summary\n",
    "df_metrics = pd.read_csv(PROCESSED_DATA_DIR / 'track_metrics.csv')\n",
    "df_thrashing = pd.read_csv(PROCESSED_DATA_DIR / 'thrashing_data.csv')\n",
    "\n",
    "# Create comprehensive summary report\n",
    "report_lines = []\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(\"C. ELEGANS MOTILITY ANALYSIS - ARCHIVE SUMMARY REPORT\")\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(f\"\\nArchive Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "report_lines.append(f\"Archive ID: {TIMESTAMP}\")\n",
    "report_lines.append(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Data summary\n",
    "report_lines.append(\"\\nDATA SUMMARY\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Total tracks analyzed: {len(df_metrics)}\")\n",
    "report_lines.append(f\"Tracks with thrashing data: {len(df_thrashing)}\")\n",
    "report_lines.append(f\"\\nGenotypes analyzed:\")\n",
    "\n",
    "genotypes = sorted(df_metrics['genotype'].unique())\n",
    "for genotype in genotypes:\n",
    "    n_tracks = len(df_metrics[df_metrics['genotype'] == genotype])\n",
    "    n_videos = df_metrics[df_metrics['genotype'] == genotype]['video_id'].nunique()\n",
    "    report_lines.append(f\"  - {genotype}: {n_tracks} tracks from {n_videos} video(s)\")\n",
    "\n",
    "# Speed summary\n",
    "report_lines.append(\"\\n\" + \"=\"*80)\n",
    "report_lines.append(\"MEAN SPEED SUMMARY (μm/s)\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "speed_summary = df_metrics.groupby('genotype')['mean_speed'].agg(['mean', 'std', 'count'])\n",
    "speed_summary['sem'] = speed_summary['std'] / np.sqrt(speed_summary['count'])\n",
    "for genotype in genotypes:\n",
    "    stats = speed_summary.loc[genotype]\n",
    "    report_lines.append(\n",
    "        f\"  {genotype}: {stats['mean']:.2f} ± {stats['sem']:.2f} μm/s (n={int(stats['count'])})\"\n",
    "    )\n",
    "\n",
    "# Straightness summary\n",
    "report_lines.append(\"\\n\" + \"=\"*80)\n",
    "report_lines.append(\"STRAIGHTNESS INDEX SUMMARY\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "straightness_summary = df_metrics.groupby('genotype')['straightness'].agg(['mean', 'std', 'count'])\n",
    "straightness_summary['sem'] = straightness_summary['std'] / np.sqrt(straightness_summary['count'])\n",
    "for genotype in genotypes:\n",
    "    stats = straightness_summary.loc[genotype]\n",
    "    report_lines.append(\n",
    "        f\"  {genotype}: {stats['mean']:.3f} ± {stats['sem']:.3f} (n={int(stats['count'])})\"\n",
    "    )\n",
    "\n",
    "# Fatigue summary\n",
    "report_lines.append(\"\\n\" + \"=\"*80)\n",
    "report_lines.append(\"FATIGUE INDEX SUMMARY\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "fatigue_summary = df_metrics.groupby('genotype')['fatigue_index'].agg(['mean', 'std', 'count'])\n",
    "fatigue_summary['sem'] = fatigue_summary['std'] / np.sqrt(fatigue_summary['count'])\n",
    "for genotype in genotypes:\n",
    "    stats = fatigue_summary.loc[genotype]\n",
    "    report_lines.append(\n",
    "        f\"  {genotype}: {stats['mean']:.3f} ± {stats['sem']:.3f} (n={int(stats['count'])})\"\n",
    "    )\n",
    "\n",
    "# Thrashing summary\n",
    "if len(df_thrashing) > 0:\n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    report_lines.append(\"THRASHING FREQUENCY SUMMARY (Hz)\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    thrashing_summary = df_thrashing.groupby('genotype')['thrashing_frequency_hz'].agg(['mean', 'std', 'count'])\n",
    "    thrashing_summary['sem'] = thrashing_summary['std'] / np.sqrt(thrashing_summary['count'])\n",
    "    for genotype in sorted(df_thrashing['genotype'].unique()):\n",
    "        if genotype in thrashing_summary.index:\n",
    "            stats = thrashing_summary.loc[genotype]\n",
    "            report_lines.append(\n",
    "                f\"  {genotype}: {stats['mean']:.3f} ± {stats['sem']:.3f} Hz (n={int(stats['count'])})\"\n",
    "            )\n",
    "\n",
    "# Files archived\n",
    "report_lines.append(\"\\n\" + \"=\"*80)\n",
    "report_lines.append(\"ARCHIVED FILES\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(\"\\nProcessed Data:\")\n",
    "for f in (ARCHIVE_SESSION_DIR / 'processed_data').glob('*'):\n",
    "    report_lines.append(f\"  - {f.name}\")\n",
    "\n",
    "report_lines.append(\"\\nResults:\")\n",
    "for f in (ARCHIVE_SESSION_DIR / 'results').glob('*'):\n",
    "    report_lines.append(f\"  - {f.name}\")\n",
    "\n",
    "report_lines.append(\"\\nFigures:\")\n",
    "for f in (ARCHIVE_SESSION_DIR / 'figures').glob('*'):\n",
    "    report_lines.append(f\"  - {f.name}\")\n",
    "\n",
    "report_lines.append(\"\\nMetadata Files:\")\n",
    "for f in (ARCHIVE_SESSION_DIR / 'metadata').glob('*'):\n",
    "    report_lines.append(f\"  - {f.name}\")\n",
    "\n",
    "report_lines.append(\"\\n\" + \"=\"*80)\n",
    "report_lines.append(\"END OF REPORT\")\n",
    "report_lines.append(\"=\"*80)\n",
    "\n",
    "# Save report\n",
    "report_text = \"\\n\".join(report_lines)\n",
    "report_path = ARCHIVE_SESSION_DIR / 'ANALYSIS_SUMMARY_REPORT.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(report_text)\n",
    "print(f\"\\n\\nReport saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe3190",
   "metadata": {},
   "source": [
    "## 7. Create Archive Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fe5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed manifest of all archived files\n",
    "manifest = {\n",
    "    'archive_info': {\n",
    "        'timestamp': TIMESTAMP,\n",
    "        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'archive_directory': str(ARCHIVE_SESSION_DIR)\n",
    "    },\n",
    "    'data_summary': {\n",
    "        'total_tracks': len(df_metrics),\n",
    "        'total_thrashing_tracks': len(df_thrashing),\n",
    "        'genotypes': genotypes,\n",
    "        'tracks_per_genotype': df_metrics['genotype'].value_counts().to_dict()\n",
    "    },\n",
    "    'archived_files': {\n",
    "        'processed_data': [f.name for f in (ARCHIVE_SESSION_DIR / 'processed_data').glob('*')],\n",
    "        'results': [f.name for f in (ARCHIVE_SESSION_DIR / 'results').glob('*')],\n",
    "        'figures': [f.name for f in (ARCHIVE_SESSION_DIR / 'figures').glob('*')],\n",
    "        'metadata': [f.name for f in (ARCHIVE_SESSION_DIR / 'metadata').glob('*')]\n",
    "    },\n",
    "    'notebooks': {\n",
    "        'data_wrangling': '01_Data_Wrangling.ipynb',\n",
    "        'analysis_plotting': '02_Analysis_and_Plotting.ipynb',\n",
    "        'archiving': '03_Archiving.ipynb'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save manifest as JSON\n",
    "manifest_path = ARCHIVE_SESSION_DIR / 'archive_manifest.json'\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(\"Archive manifest created:\")\n",
    "print(json.dumps(manifest, indent=2))\n",
    "print(f\"\\nManifest saved to: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ecd8f",
   "metadata": {},
   "source": [
    "## 8. Create README for Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README file for the archive\n",
    "readme_content = f\"\"\"# C. elegans Motility Analysis Archive\n",
    "\n",
    "**Archive ID:** {TIMESTAMP}  \n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Overview\n",
    "\n",
    "This archive contains the complete analysis pipeline and results for C. elegans motility analysis.\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```\n",
    "archive_{TIMESTAMP}/\n",
    "├── processed_data/       # Processed track data, metrics, and thrashing data\n",
    "├── results/              # Statistical summaries and test results\n",
    "├── figures/              # Publication-quality figures (PNG and PDF)\n",
    "├── metadata/             # Experimental metadata from YAML files\n",
    "├── ANALYSIS_SUMMARY_REPORT.txt  # Comprehensive summary report\n",
    "├── archive_manifest.json        # Detailed file manifest\n",
    "└── README.md                     # This file\n",
    "```\n",
    "\n",
    "## Analysis Pipeline\n",
    "\n",
    "The analysis was performed using three Jupyter notebooks:\n",
    "\n",
    "1. **01_Data_Wrangling.ipynb** - Data loading, processing, and metric calculation\n",
    "2. **02_Analysis_and_Plotting.ipynb** - Statistical analysis and visualization\n",
    "3. **03_Archiving.ipynb** - Data archiving and documentation\n",
    "\n",
    "## Data Summary\n",
    "\n",
    "- **Total tracks analyzed:** {len(df_metrics)}\n",
    "- **Tracks with thrashing data:** {len(df_thrashing)}\n",
    "- **Genotypes:** {', '.join(genotypes)}\n",
    "\n",
    "## Metrics Calculated\n",
    "\n",
    "1. **Mean Speed** - Average movement speed (μm/s)\n",
    "2. **Straightness Index** - Ratio of displacement to path length (0-1)\n",
    "3. **Fatigue Index** - Ratio of late-phase to early-phase speed\n",
    "4. **Thrashing Frequency** - Body oscillation frequency (Hz)\n",
    "\n",
    "## Statistical Tests\n",
    "\n",
    "- One-way ANOVA for overall group differences\n",
    "- Pairwise t-tests for genotype comparisons\n",
    "- Mean ± SEM reported for all metrics\n",
    "\n",
    "## Files Included\n",
    "\n",
    "### Processed Data\n",
    "- `track_metrics.csv` - Per-track metrics (speed, straightness, fatigue, etc.)\n",
    "- `thrashing_data.csv` - Thrashing frequency data\n",
    "- `normalized_tracks.csv` - XY coordinates normalized to origin\n",
    "\n",
    "### Results\n",
    "- `*_group_summary.csv` - Summary statistics by genotype\n",
    "- `*_pairwise_tests.csv` - Statistical test results\n",
    "- `stats_summary.json` - Comprehensive statistical summary\n",
    "\n",
    "### Figures\n",
    "- `mean_speed_comparison.*` - Speed comparison by genotype\n",
    "- `straightness_comparison.*` - Straightness index comparison\n",
    "- `fatigue_comparison.*` - Fatigue index comparison\n",
    "- `thrashing_frequency_comparison.*` - Thrashing frequency comparison\n",
    "- `track_trajectories_*.* ` - XY trajectory plots\n",
    "- `summary_figure.*` - Multi-panel summary figure\n",
    "\n",
    "## Citation\n",
    "\n",
    "If using this data, please cite:\n",
    "- Analysis pipeline: Custom C. elegans motility analysis (Notebooks 01-03)\n",
    "- WormLab: MBF Bioscience (https://www.mbfbioscience.com/wormlab)\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions about this analysis, please refer to the experimental metadata files.\n",
    "\n",
    "---\n",
    "\n",
    "*Archive generated automatically by 03_Archiving.ipynb*\n",
    "\"\"\"\n",
    "\n",
    "readme_path = ARCHIVE_SESSION_DIR / 'README.md'\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README created successfully!\")\n",
    "print(f\"\\nSaved to: {readme_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c5b1f",
   "metadata": {},
   "source": [
    "## 9. Optional: Create ZIP Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df546518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a compressed ZIP file of the entire archive\n",
    "zip_path = ARCHIVE_DIR / f'archive_{TIMESTAMP}.zip'\n",
    "\n",
    "print(f\"Creating ZIP archive: {zip_path.name}\")\n",
    "print(\"This may take a moment...\\n\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in ARCHIVE_SESSION_DIR.rglob('*'):\n",
    "        if file_path.is_file():\n",
    "            arc_name = file_path.relative_to(ARCHIVE_SESSION_DIR.parent)\n",
    "            zipf.write(file_path, arc_name)\n",
    "            print(f\"  Added: {arc_name}\")\n",
    "\n",
    "zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"\\nZIP archive created successfully!\")\n",
    "print(f\"Size: {zip_size_mb:.2f} MB\")\n",
    "print(f\"Location: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205b7f6",
   "metadata": {},
   "source": [
    "## 10. Clean Up Old Archives (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a135e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all archive directories\n",
    "archive_dirs = sorted([d for d in ARCHIVE_DIR.iterdir() if d.is_dir() and d.name.startswith('archive_')])\n",
    "\n",
    "print(f\"Found {len(archive_dirs)} archive directory/directories:\")\n",
    "for archive_dir in archive_dirs:\n",
    "    creation_time = datetime.fromtimestamp(archive_dir.stat().st_ctime)\n",
    "    print(f\"  - {archive_dir.name} (created: {creation_time.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "\n",
    "# Optional: Delete archives older than X days (UNCOMMENT TO ENABLE)\n",
    "# DAYS_TO_KEEP = 30\n",
    "# cutoff_date = datetime.now() - pd.Timedelta(days=DAYS_TO_KEEP)\n",
    "# \n",
    "# for archive_dir in archive_dirs:\n",
    "#     creation_time = datetime.fromtimestamp(archive_dir.stat().st_ctime)\n",
    "#     if creation_time < cutoff_date and archive_dir.name != f'archive_{TIMESTAMP}':\n",
    "#         print(f\"Deleting old archive: {archive_dir.name}\")\n",
    "#         shutil.rmtree(archive_dir)\n",
    "\n",
    "print(\"\\nNote: Automatic cleanup is disabled by default. Edit this cell to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf3068",
   "metadata": {},
   "source": [
    "## 11. Archive Summary and Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ARCHIVING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nArchive ID: {TIMESTAMP}\")\n",
    "print(f\"Archive Location: {ARCHIVE_SESSION_DIR}\")\n",
    "\n",
    "print(\"\\nArchived Contents:\")\n",
    "print(f\"  - Processed data files: {len(list((ARCHIVE_SESSION_DIR / 'processed_data').glob('*')))}\")\n",
    "print(f\"  - Result files: {len(list((ARCHIVE_SESSION_DIR / 'results').glob('*')))}\")\n",
    "print(f\"  - Figure files: {len(list((ARCHIVE_SESSION_DIR / 'figures').glob('*')))}\")\n",
    "print(f\"  - Metadata files: {len(list((ARCHIVE_SESSION_DIR / 'metadata').glob('*')))}\")\n",
    "\n",
    "print(\"\\nDocumentation:\")\n",
    "print(f\"  ✓ README.md\")\n",
    "print(f\"  ✓ ANALYSIS_SUMMARY_REPORT.txt\")\n",
    "print(f\"  ✓ archive_manifest.json\")\n",
    "\n",
    "if zip_path.exists():\n",
    "    print(f\"\\nCompressed Archive:\")\n",
    "    print(f\"  ✓ {zip_path.name} ({zip_size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All analysis results have been archived and documented.\")\n",
    "print(\"The archive is ready for long-term storage or sharing.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
