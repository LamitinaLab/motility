{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d375b94",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Wrangling for C. elegans Motility Analysis\n",
    "\n",
    "This notebook handles:\n",
    "- Loading raw WormLab data (Position, Fit, CurvatureMap files)\n",
    "- Reading metadata from YAML files\n",
    "- Processing track data and calculating metrics\n",
    "- Normalizing track positions to origin (0,0)\n",
    "- Calculating instantaneous speeds\n",
    "- Computing movement metrics (straightness index, path length, etc.)\n",
    "- Extracting thrashing frequency from curvature data\n",
    "- Saving processed data for downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa1c75",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf17ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122175e",
   "metadata": {},
   "source": [
    "## 2. Configuration and Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd9c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: C:\\Users\\MBF\\Motility_analysis\n",
      "Raw data directory: C:\\Users\\MBF\\Motility_analysis\\Data\\Raw\n",
      "Processed data directory: C:\\Users\\MBF\\Motility_analysis\\Data\\Processed\n",
      "Results directory: C:\\Users\\MBF\\Motility_analysis\\results\n"
     ]
    }
   ],
   "source": [
    "# Base paths\n",
    "BASE_DIR = Path(r'C:\\Users\\MBF\\Motility_analysis')\n",
    "RAW_DATA_DIR = BASE_DIR / 'Data' / 'Raw'\n",
    "PROCESSED_DATA_DIR = BASE_DIR / 'Data' / 'Processed'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc8989",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5492001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_metadata(yaml_path):\n",
    "    \"\"\"\n",
    "    Load metadata from YAML file\n",
    "    \"\"\"\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        metadata = yaml.safe_load(f)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_position_file(position_path):\n",
    "    \"\"\"\n",
    "    Load Position CSV file, skipping first row (track labels) and using second row as header\n",
    "    Returns: DataFrame with Frame, Time, and X/Y coordinate pairs for each track\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(position_path, skiprows=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_track_data(position_df):\n",
    "    \"\"\"\n",
    "    Extract individual track data from Position DataFrame\n",
    "    Returns: Dictionary of DataFrames, one per track\n",
    "    \"\"\"\n",
    "    tracks = {}\n",
    "    \n",
    "    # Get column names excluding Frame and Time\n",
    "    columns = position_df.columns.tolist()\n",
    "    \n",
    "    # Extract track numbers from column names (format: \"1 x\", \"1 y\", \"2 x\", \"2 y\", etc.)\n",
    "    track_nums = set()\n",
    "    for col in columns:\n",
    "        if col not in ['Frame', 'Time']:\n",
    "            # Extract track number from column name\n",
    "            track_num = col.split()[0]\n",
    "            track_nums.add(track_num)\n",
    "    \n",
    "    # Create DataFrame for each track\n",
    "    for track_num in sorted(track_nums):\n",
    "        x_col = f\"{track_num} x\"\n",
    "        y_col = f\"{track_num} y\"\n",
    "        \n",
    "        if x_col in columns and y_col in columns:\n",
    "            track_df = pd.DataFrame({\n",
    "                'Frame': position_df['Frame'],\n",
    "                'Time': position_df['Time'],\n",
    "                'X': position_df[x_col],\n",
    "                'Y': position_df[y_col]\n",
    "            })\n",
    "            \n",
    "            # Remove NaN values\n",
    "            track_df = track_df.dropna()\n",
    "            \n",
    "            if len(track_df) > 0:\n",
    "                tracks[track_num] = track_df\n",
    "    \n",
    "    return tracks\n",
    "\n",
    "\n",
    "def normalize_track_to_origin(track_df):\n",
    "    \"\"\"\n",
    "    Normalize track so it starts at (0, 0)\n",
    "    \"\"\"\n",
    "    track_normalized = track_df.copy()\n",
    "    track_normalized['X'] = track_df['X'] - track_df['X'].iloc[0]\n",
    "    track_normalized['Y'] = track_df['Y'] - track_df['Y'].iloc[0]\n",
    "    return track_normalized\n",
    "\n",
    "\n",
    "def calculate_speed(track_df):\n",
    "    \"\"\"\n",
    "    Calculate instantaneous speed from position data\n",
    "    Speed in microns per second\n",
    "    \"\"\"\n",
    "    track_df = track_df.copy()\n",
    "    \n",
    "    # Calculate displacement\n",
    "    dx = track_df['X'].diff()\n",
    "    dy = track_df['Y'].diff()\n",
    "    dt = track_df['Time'].diff()\n",
    "    \n",
    "    # Calculate distance and speed\n",
    "    distance = np.sqrt(dx**2 + dy**2)\n",
    "    speed = distance / dt\n",
    "    \n",
    "    track_df['Speed'] = speed\n",
    "    \n",
    "    return track_df\n",
    "\n",
    "\n",
    "def calculate_path_metrics(track_df):\n",
    "    \"\"\"\n",
    "    Calculate various path metrics:\n",
    "    - Total path length\n",
    "    - Displacement (straight-line distance from start to end)\n",
    "    - Straightness index (displacement / path length)\n",
    "    - Mean speed\n",
    "    \"\"\"\n",
    "    # Calculate distances between consecutive points\n",
    "    dx = track_df['X'].diff()\n",
    "    dy = track_df['Y'].diff()\n",
    "    distances = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    # Total path length\n",
    "    path_length = distances.sum()\n",
    "    \n",
    "    # Displacement (start to end)\n",
    "    displacement = np.sqrt(\n",
    "        (track_df['X'].iloc[-1] - track_df['X'].iloc[0])**2 +\n",
    "        (track_df['Y'].iloc[-1] - track_df['Y'].iloc[0])**2\n",
    "    )\n",
    "    \n",
    "    # Straightness index\n",
    "    straightness = displacement / path_length if path_length > 0 else 0\n",
    "    \n",
    "    # Mean speed (excluding NaN values)\n",
    "    if 'Speed' in track_df.columns:\n",
    "        mean_speed = track_df['Speed'].dropna().mean()\n",
    "    else:\n",
    "        mean_speed = np.nan\n",
    "    \n",
    "    return {\n",
    "        'path_length': path_length,\n",
    "        'displacement': displacement,\n",
    "        'straightness': straightness,\n",
    "        'mean_speed': mean_speed\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_fatigue_index(track_df, window_size=50):\n",
    "    \"\"\"\n",
    "    Calculate fatigue index as ratio of speed in last window vs first window\n",
    "    Values < 1 indicate slowing down (fatigue)\n",
    "    \"\"\"\n",
    "    if 'Speed' not in track_df.columns or len(track_df) < 2 * window_size:\n",
    "        return np.nan\n",
    "    \n",
    "    first_window_speed = track_df['Speed'].iloc[:window_size].mean()\n",
    "    last_window_speed = track_df['Speed'].iloc[-window_size:].mean()\n",
    "    \n",
    "    if first_window_speed > 0:\n",
    "        fatigue_index = last_window_speed / first_window_speed\n",
    "    else:\n",
    "        fatigue_index = np.nan\n",
    "    \n",
    "    return fatigue_index\n",
    "\n",
    "\n",
    "def load_curvature_file(curvature_path):\n",
    "    \"\"\"\n",
    "    Load CurvatureMap CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(curvature_path, skiprows=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_thrashing_frequency(curvature_df, midbody_column=None):\n",
    "    \"\"\"\n",
    "    Calculate thrashing frequency from curvature data\n",
    "    Uses midbody curvature oscillations to detect thrashing\n",
    "    Returns frequency in Hz\n",
    "    \"\"\"\n",
    "    # If no specific column specified, use middle column (midbody)\n",
    "    curvature_columns = [col for col in curvature_df.columns if col not in ['Frame', 'Time']]\n",
    "    \n",
    "    if len(curvature_columns) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    if midbody_column is None:\n",
    "        # Use middle column as midbody\n",
    "        midbody_column = curvature_columns[len(curvature_columns) // 2]\n",
    "    \n",
    "    # Extract midbody curvature\n",
    "    curvature = curvature_df[midbody_column].dropna()\n",
    "    time = curvature_df['Time'].loc[curvature.index]\n",
    "    \n",
    "    if len(curvature) < 10:\n",
    "        return np.nan\n",
    "    \n",
    "    # Find peaks in curvature (thrashing events)\n",
    "    # Use prominence to identify significant oscillations\n",
    "    peaks, properties = find_peaks(curvature, prominence=50)\n",
    "    \n",
    "    if len(peaks) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate frequency\n",
    "    total_time = time.iloc[-1] - time.iloc[0]\n",
    "    frequency = len(peaks) / total_time  # Hz\n",
    "    \n",
    "    return frequency\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca56aa",
   "metadata": {},
   "source": [
    "## 4. Discover Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c24f29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 date folder(s):\n",
      "  - 260224\n",
      "  NOTE: 260224 has no Wormlab_processed folder; using direct subfolders.\n",
      "  260224/OGxxxx_1mMaux: 4 video(s)\n",
      "  260224/OGxxxx_noaux: 4 video(s)\n",
      "\n",
      "Unique genotypes found: ['OGxxxx_1mMaux', 'OGxxxx_noaux']\n"
     ]
    }
   ],
   "source": [
    "# Find all dates (subdirectories in Raw)\n",
    "dates = [d for d in RAW_DATA_DIR.iterdir() if d.is_dir()]\n",
    "\n",
    "print(f\"Found {len(dates)} date folder(s):\")\n",
    "for date_dir in dates:\n",
    "    print(f\"  - {date_dir.name}\")\n",
    "\n",
    "# Find all genotypes across all dates\n",
    "all_genotypes = []\n",
    "data_structure = {}\n",
    "\n",
    "for date_dir in dates:\n",
    "    wormlab_dir = date_dir / 'Wormlab_processed'\n",
    "    if wormlab_dir.exists():\n",
    "        genotype_roots = [g for g in wormlab_dir.iterdir() if g.is_dir()]\n",
    "    else:\n",
    "        genotype_roots = [g for g in date_dir.iterdir() if g.is_dir()]\n",
    "        print(f\"  NOTE: {date_dir.name} has no Wormlab_processed folder; using direct subfolders.\")\n",
    "\n",
    "    data_structure[date_dir.name] = {}\n",
    "\n",
    "    for genotype_dir in genotype_roots:\n",
    "        genotype_name = genotype_dir.name\n",
    "        all_genotypes.append(genotype_name)\n",
    "\n",
    "        # Count videos for this genotype\n",
    "        videos = [v for v in genotype_dir.iterdir() if v.is_dir() and v.name.startswith('000')]\n",
    "        data_structure[date_dir.name][genotype_name] = len(videos)\n",
    "\n",
    "        print(f\"  {date_dir.name}/{genotype_name}: {len(videos)} video(s)\")\n",
    "\n",
    "unique_genotypes = sorted(set(all_genotypes))\n",
    "print(f\"\\nUnique genotypes found: {unique_genotypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfccf2",
   "metadata": {},
   "source": [
    "## 5. Load and Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd6e04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: 260224 has no Wormlab_processed folder; using direct subfolders.\n",
      "\n",
      "Processing 260224/OGxxxx_1mMaux...\n",
      "  Processing video 0001...\n",
      "    Found 10 track(s)\n",
      "  Processing video 0002...\n",
      "    Found 5 track(s)\n",
      "  Processing video 0003...\n",
      "    Found 5 track(s)\n",
      "  Processing video 0004...\n",
      "    Found 9 track(s)\n",
      "\n",
      "Processing 260224/OGxxxx_noaux...\n",
      "  Processing video 0001...\n",
      "    Found 6 track(s)\n",
      "  Processing video 0002...\n",
      "    Found 6 track(s)\n",
      "  Processing video 0003...\n",
      "    Found 11 track(s)\n",
      "  Processing video 0004...\n",
      "    Found 12 track(s)\n",
      "\n",
      "\n",
      "DATA PROCESSING COMPLETE!\n",
      "Total tracks processed: 64\n",
      "Tracks with thrashing data: 64\n"
     ]
    }
   ],
   "source": [
    "# Master data storage\n",
    "all_track_data = []  # List of dictionaries with all track info\n",
    "all_track_coordinates = []  # For XY plotting\n",
    "all_metrics = []  # Summary metrics per track\n",
    "all_thrashing = []  # Thrashing frequency data\n",
    "\n",
    "# Process each date\n",
    "for date_dir in dates:\n",
    "    date_name = date_dir.name\n",
    "    wormlab_dir = date_dir / 'Wormlab_processed'\n",
    "\n",
    "    if wormlab_dir.exists():\n",
    "        genotype_roots = [g for g in wormlab_dir.iterdir() if g.is_dir()]\n",
    "    else:\n",
    "        genotype_roots = [g for g in date_dir.iterdir() if g.is_dir()]\n",
    "        print(f\"\\nNOTE: {date_name} has no Wormlab_processed folder; using direct subfolders.\")\n",
    "\n",
    "    # Process each genotype\n",
    "    for genotype_dir in genotype_roots:\n",
    "        genotype_name = genotype_dir.name\n",
    "        print(f\"\\nProcessing {date_name}/{genotype_name}...\")\n",
    "\n",
    "        # Load metadata if available\n",
    "        metadata_files = list(genotype_dir.glob('metadata_*.yaml'))\n",
    "        metadata = {}\n",
    "        if metadata_files:\n",
    "            metadata = load_metadata(metadata_files[0])\n",
    "            print(f\"  Loaded metadata: {metadata.get('strain', 'N/A')}, genotype: {metadata.get('genotype', 'N/A')}\")\n",
    "        auxin_mM = metadata.get('auxin_mM', np.nan)\n",
    "\n",
    "        # Process each video\n",
    "        videos = [v for v in genotype_dir.iterdir() if v.is_dir() and v.name.startswith('000')]\n",
    "        if not videos:\n",
    "            print(f\"  WARNING: No video folders (000*) found in {genotype_dir}\")\n",
    "            continue\n",
    "\n",
    "        for video_dir in sorted(videos):\n",
    "            video_id = video_dir.name\n",
    "            print(f\"  Processing video {video_id}...\")\n",
    "\n",
    "            # Find Position file (may be Position.csv or Position-1.csv, etc.)\n",
    "            position_files = list(video_dir.glob('Position*.csv'))\n",
    "            if not position_files:\n",
    "                print(f\"    WARNING: No Position file found in {video_dir}\")\n",
    "                continue\n",
    "\n",
    "            position_file = position_files[0]\n",
    "\n",
    "            try:\n",
    "                # Load position data\n",
    "                position_df = load_position_file(position_file)\n",
    "\n",
    "                # Extract individual tracks\n",
    "                tracks = extract_track_data(position_df)\n",
    "                print(f\"    Found {len(tracks)} track(s)\")\n",
    "\n",
    "                # Process each track\n",
    "                for track_num, track_df in tracks.items():\n",
    "                    # Calculate speed\n",
    "                    track_df = calculate_speed(track_df)\n",
    "\n",
    "                    # Normalize to origin\n",
    "                    track_normalized = normalize_track_to_origin(track_df)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    metrics = calculate_path_metrics(track_df)\n",
    "                    fatigue = calculate_fatigue_index(track_df)\n",
    "\n",
    "                    # Store track data\n",
    "                    track_info = {\n",
    "                        'date': date_name,\n",
    "                        'genotype': genotype_name,\n",
    "                        'strain': metadata.get('strain', genotype_name),\n",
    "                        'auxin_mM': auxin_mM,\n",
    "                        'video_id': video_id,\n",
    "                        'track_num': track_num,\n",
    "                        'track_id': f\"{genotype_name}_{video_id}_{track_num}\",\n",
    "                        'mean_speed': metrics['mean_speed'],\n",
    "                        'path_length': metrics['path_length'],\n",
    "                        'displacement': metrics['displacement'],\n",
    "                        'straightness': metrics['straightness'],\n",
    "                        'fatigue_index': fatigue,\n",
    "                        'n_frames': len(track_df),\n",
    "                        'duration': track_df['Time'].iloc[-1] - track_df['Time'].iloc[0]\n",
    "                    }\n",
    "\n",
    "                    all_metrics.append(track_info)\n",
    "\n",
    "                    # Store normalized coordinates for plotting\n",
    "                    track_normalized['genotype'] = genotype_name\n",
    "                    track_normalized['auxin_mM'] = auxin_mM\n",
    "                    track_normalized['track_id'] = track_info['track_id']\n",
    "                    all_track_coordinates.append(track_normalized)\n",
    "\n",
    "                    # Process curvature data for thrashing\n",
    "                    curvature_files = list(video_dir.glob(f'CurvatureMap*Track_{track_num}.csv'))\n",
    "                    if curvature_files:\n",
    "                        try:\n",
    "                            curvature_df = load_curvature_file(curvature_files[0])\n",
    "                            thrashing_freq = calculate_thrashing_frequency(curvature_df)\n",
    "\n",
    "                            thrashing_info = {\n",
    "                                'date': date_name,\n",
    "                                'genotype': genotype_name,\n",
    "                                'strain': metadata.get('strain', genotype_name),\n",
    "                                'auxin_mM': auxin_mM,\n",
    "                                'video_id': video_id,\n",
    "                                'track_num': track_num,\n",
    "                                'track_id': track_info['track_id'],\n",
    "                                'thrashing_frequency_hz': thrashing_freq\n",
    "                            }\n",
    "\n",
    "                            all_thrashing.append(thrashing_info)\n",
    "                        except Exception as e:\n",
    "                            print(f\"      WARNING: Error processing curvature for track {track_num}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR processing {position_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "print(f\"\\n\\nDATA PROCESSING COMPLETE!\")\n",
    "print(f\"Total tracks processed: {len(all_metrics)}\")\n",
    "print(f\"Tracks with thrashing data: {len(all_thrashing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b7229",
   "metadata": {},
   "source": [
    "## 6. Create Summary DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7732cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== METRICS SUMMARY ===\n",
      "     date       genotype         strain video_id track_num  \\\n",
      "0  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         1   \n",
      "1  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001        10   \n",
      "2  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         2   \n",
      "3  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         3   \n",
      "4  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         4   \n",
      "\n",
      "                track_id  mean_speed   path_length  displacement  \\\n",
      "0   OGxxxx_1mMaux_0001_1  473.304367  14165.323551   4110.763155   \n",
      "1  OGxxxx_1mMaux_0001_10  744.466036   1500.690950    686.124115   \n",
      "2   OGxxxx_1mMaux_0001_2  465.642803  13985.680807   2055.968505   \n",
      "3   OGxxxx_1mMaux_0001_3  455.841449  13642.683369   1966.355430   \n",
      "4   OGxxxx_1mMaux_0001_4  476.374636  14257.212325    210.681784   \n",
      "\n",
      "   straightness  fatigue_index  n_frames   duration  \n",
      "0      0.290199       0.948489       420  29.928571  \n",
      "1      0.457205            NaN        27   1.928571  \n",
      "2      0.147005       1.018432       416  29.928571  \n",
      "3      0.144133       0.994060       420  29.928571  \n",
      "4      0.014777       1.197738       420  29.928571  \n",
      "\n",
      "Shape: (64, 13)\n",
      "\n",
      "=== THRASHING SUMMARY ===\n",
      "     date       genotype         strain video_id track_num  \\\n",
      "0  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         1   \n",
      "1  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001        10   \n",
      "2  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         2   \n",
      "3  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         3   \n",
      "4  260224  OGxxxx_1mMaux  OGxxxx_1mMaux     0001         4   \n",
      "\n",
      "                track_id  thrashing_frequency_hz  \n",
      "0   OGxxxx_1mMaux_0001_1                1.837709  \n",
      "1  OGxxxx_1mMaux_0001_10                2.074074  \n",
      "2   OGxxxx_1mMaux_0001_2                1.804296  \n",
      "3   OGxxxx_1mMaux_0001_3                1.871122  \n",
      "4   OGxxxx_1mMaux_0001_4                1.670644  \n",
      "\n",
      "Shape: (64, 7)\n",
      "\n",
      "=== TRACK COORDINATES SUMMARY ===\n",
      "   Frame      Time           X          Y       Speed       genotype  \\\n",
      "0      1  0.000000    0.000000   0.000000         NaN  OGxxxx_1mMaux   \n",
      "1      2  0.071429   45.032517   5.201520  634.646948  OGxxxx_1mMaux   \n",
      "2      3  0.142857   98.833971   4.047367  753.393642  OGxxxx_1mMaux   \n",
      "3      4  0.214286  129.841225  -5.367656  453.671819  OGxxxx_1mMaux   \n",
      "4      5  0.285714  127.349672 -31.032366  360.995153  OGxxxx_1mMaux   \n",
      "\n",
      "               track_id  \n",
      "0  OGxxxx_1mMaux_0001_1  \n",
      "1  OGxxxx_1mMaux_0001_1  \n",
      "2  OGxxxx_1mMaux_0001_1  \n",
      "3  OGxxxx_1mMaux_0001_1  \n",
      "4  OGxxxx_1mMaux_0001_1  \n",
      "\n",
      "Shape: (19238, 7)\n",
      "\n",
      "=== SUMMARY BY GENOTYPE ===\n",
      "               mean_speed                   straightness            \\\n",
      "                     mean         std count         mean       std   \n",
      "genotype                                                             \n",
      "OGxxxx_1mMaux  484.884694   76.658417    29     0.191471  0.174029   \n",
      "OGxxxx_noaux   467.974075  129.228456    35     0.177663  0.171799   \n",
      "\n",
      "              fatigue_index            \n",
      "                       mean       std  \n",
      "genotype                               \n",
      "OGxxxx_1mMaux      1.044339  0.274158  \n",
      "OGxxxx_noaux       0.977340  0.212718  \n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrames\n",
    "df_metrics = pd.DataFrame(all_metrics)\n",
    "df_thrashing = pd.DataFrame(all_thrashing)\n",
    "df_tracks = pd.concat(all_track_coordinates, ignore_index=True) if all_track_coordinates else pd.DataFrame()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== METRICS SUMMARY ===\")\n",
    "print(df_metrics.head())\n",
    "print(f\"\\nShape: {df_metrics.shape}\")\n",
    "\n",
    "print(\"\\n=== THRASHING SUMMARY ===\")\n",
    "print(df_thrashing.head())\n",
    "print(f\"\\nShape: {df_thrashing.shape}\")\n",
    "\n",
    "print(\"\\n=== TRACK COORDINATES SUMMARY ===\")\n",
    "print(df_tracks.head())\n",
    "print(f\"\\nShape: {df_tracks.shape}\")\n",
    "\n",
    "# Summary by genotype\n",
    "print(\"\\n=== SUMMARY BY GENOTYPE ===\")\n",
    "summary_by_genotype = df_metrics.groupby('genotype').agg({\n",
    "    'mean_speed': ['mean', 'std', 'count'],\n",
    "    'straightness': ['mean', 'std'],\n",
    "    'fatigue_index': ['mean', 'std']\n",
    "})\n",
    "print(summary_by_genotype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb94a3c",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5caeabea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to:\n",
      "  - C:\\Users\\MBF\\Motility_analysis\\Data\\Processed\\track_metrics.csv\n",
      "  - C:\\Users\\MBF\\Motility_analysis\\Data\\Processed\\thrashing_data.csv\n",
      "  - C:\\Users\\MBF\\Motility_analysis\\Data\\Processed\\normalized_tracks.csv\n",
      "\n",
      "✓ DATA WRANGLING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV files\n",
    "df_metrics.to_csv(PROCESSED_DATA_DIR / 'track_metrics.csv', index=False)\n",
    "df_thrashing.to_csv(PROCESSED_DATA_DIR / 'thrashing_data.csv', index=False)\n",
    "df_tracks.to_csv(PROCESSED_DATA_DIR / 'normalized_tracks.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved to:\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'track_metrics.csv'}\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'thrashing_data.csv'}\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'normalized_tracks.csv'}\")\n",
    "\n",
    "print(\"\\n✓ DATA WRANGLING COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
